{"default_url":null,"content":{"state":2,"taskInstructions":"<h2><b>Instructions </b></h2><br>See verification instructions.<br>","references":[],"isTaskInstructionsVisible":true,"isCommentMandatory":false,"fields":[],"masterFinding":{"id":17064031,"exploitInstructions":"","verificationInstructions":"<h2><b>Instructions </b></h2><br>1. Identify bucket names by scanning in scope application for references, Google dorking for references, and guessing/permutating names.<br>2. Check permissions on the bucket to see if listing or read/write is allowed, and for which type of user (authenticated users or all/unauthenticated users).<br>3. Review files in the bucket for sensitive data.<br><br><h2>Variation: Bucket Enumeration - cloud_enum</h2><br>Reference: https://github.com/initstring/cloud_enum<br><br>1. Download cloud_enum and follow the installation instructions in the repository.<br><code>git clone https://github.com/initstring/cloud_enum.git<br></code><br>2. Run the following (or similar) command.<br><code>python3 cloud_enum.py -k netspi -ns 1.1.1.1 --disable-azure --disable-gcp<br><br>&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;<br>        cloud_enum<br>   github.com/initstring<br>&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;&#35;<br><br><br>Keywords:    netspi<br>Mutations:   /home/ncroy/tools/cloud_enum/enum_tools/fuzz.txt<br>Brute-list:  /home/ncroy/tools/cloud_enum/enum_tools/fuzz.txt<br><br>[+] Mutations list imported: 242 items<br>[+] Mutated results: 1453 items<br><br>++++++++++++++++++++++++++<br>      amazon checks<br>++++++++++++++++++++++++++<br><br>[+] Checking for S3 buckets<br>  Protected S3 Bucket: http://netspi.s3.amazonaws.com/<br>  Protected S3 Bucket: http://netspibackups.s3.amazonaws.com/<br>  Protected S3 Bucket: http://netspifiles.s3.amazonaws.com/<br>  Protected S3 Bucket: http://netspi-templates.s3.amazonaws.com/<br>  Protected S3 Bucket: http://netspitest.s3.amazonaws.com/<br><br> Elapsed time: 00:04:06<br><b>[TRUNCATED]</b><br></code><br>3. Review the tool output to see if it was able to determine that any of the buckets were open.<br><br><h2><b>Variation: Bucket Enumeration - Web Application Scanning</b></h2><br>Reference: https://github.com/NetSPI/Burp-Extensions-Private/tree/master/FindCloudServiceRefs<br><br>1. Use Burp to scan in scope web applications with the extension above installed to find AWS S3 bucket references as time allows.<br><br><h2><b>Variation: Bucket Enumeration - </b>buckets.grayhatwarfare.com</h2><br>1. Navigate to buckets.grayhatwarfare.com and search for buckets using the company, domain, and known application names.<br>2. Focus on buckets, and deprioritize file names.<br><br><h2><b>Variation: Bucket Enumeration - Google Dorking</b></h2><br>1. Perform Google search to find potential S3 bucket names for the client, domains, and apps as time allows.  <br><br><h3><b>Example:</b></h3><br><code>clientname inbody:s3.amazonaws.com <br></code><br><br><h2><b>Variation: Bucket and File Enumeration: Get-PublicAwsS3BucketList</b></h2><br>If you're unauthenticated and have the bucket name you can attempt to get a list of accessible files using the script below. It can be used to guess S3 buckets that could be associated with a client. The script does support importing a list of bucket names from a file and can perform basic permutations of provided bucket names.<br><br><b>Note:</b> Target bucket names that map to the company name, company applications, and internal/external domain names.<br><br>1. Load the script into your PowerShell session.<br><br><code>iex(new-object net.webclient).downloadstring(&quot;https://raw.githubusercontent.com/nullbind/Powershellery/master/Stable-ish/AWS/Get-PublicAwsS3BucketList.ps1&quot;)<br></code><br>2a. Target single company name, domain, or app.  For domains, consider dropping the &quot;.com&quot;.<br><code>Get-PublicAwsS3BucketListFromDomains -S3Bucket &quot;acme&quot; -Verbose<br></code><br>2b. Target single company name, domain, or app.  For domains, consider dropping the &quot;.com&quot;, but also run permutations.<br><code>Get-PublicAwsS3BucketListFromDomains -S3Bucket &quot;ameriprise&quot; -Verbose -Permutate<br></code><br>2c. Target known bucket for listing.<br><code> Get-PublicAwsS3BucketList -S3BucketName &quot;<a href='http://hq-sandbox.us-east-1.subs.static.arcpublishing.com/'>hq-sandbox.us-east-1.subs.static.acme.com</a>&quot; -Verbose <br></code><br>To store results to a variation use the command below:<br><code>$Results = Get-PublicAwsS3BucketListFromDomains -S3Bucket &quot;acme&quot; -Verbose -Permutate<br></code><br>3. View the results.<br><code>$Results<br></code><br>4. Write the results to a file and upload it to the Documents section in the Resolve project.<br><code>$Results | Export-CSV -NoTypeInformation s3-list-domains.csv<br></code><br>5.  If any of the enumerated buckets allow S3 key listing, add this finding and show the process used to list the keys. Include a list of all buckets that provide list permissions to everyone.  If sensitive data is found add separate high findings calling out unauthenticated access to sensitive data.<br><br>6. Review available listings for data targets. Group the accessible keys based on filetype, and target outliers and interesting file types such as configurations files.<br><code>$Results | Where-Object FileType -NotLike &quot;&#42;/&#42;&quot; | Group-Object FileType | Select Name,Count | Sort-Object count -Descending<br></code><br><br><h2>Checking Permissions/Files After Bucket Enumeration</h2><br><b>Authenticated Test</b><br>You can use the cli command below if you have AWS account credentials for the account that owns the bucket:<br><code>aws s3 ls s3://bucketnamehere<br></code><br>Also, you can use scout or weirdAAL with your AWS SSO or acquired keys.<br><br><ul><li>https://github.com/nccgroup/Scout2</li><br><li>https://github.com/carnal0wnage/weirdAAL</li><br><li>https://github.com/carnal0wnage/weirdAAL/wiki</li><br><li>https://github.com/cyberark/SkyArk</li><br></ul><br><b>Note:</b> Most of the time you will not have AWS credentials, so below are some instructions for finding S3 buckets and files and checking permissions using your own NetSPI AWS credentials, or completely anonymously.<br><br><b>Semi-Authenticated Test</b><br><a href='https://s3check.netspi.io/'>https://s3check.netspi.io/</a><br>Reference Code: https://github.com/NetSPI/S3PermissionChecker<br><br>Use the S3PermissionChecker website on each bucket to determine permissions. The tool authenticates you to NetSPI's AWS instance, and then checks permissions to determine if the bucket is entirely public, or if certain permissions are misconfigured to permit authenticated users access even if they're not part of the AWS account that owns the bucket. The standalone GitHub tool can be used if the website is inaccessible for any reason. Follow the setup instructions in the repository.<br><br><b>Unauthenticated Test: Manual</b><br>If you have already identified an S3 bucket, you can check for list permissions by browsing to either:<br><br>https://[subdomain].s3.amazonaws.com <br>-or-<br>https://[subdomain].s3.amazonaws.com?max-keys=1000&amp;list-type=2<br><br><h2>Additional Reporting</h2><br>If sensitive data is found during the bucket enumeration process, also add one of the findings below (in addition to the finding documenting the S3 bucket permission issues):<br><ul><li>Sensitive Information Disclosure - Cleartext Password</li><br><li>Sensitive Information Disclosure - Publicly Available Resources</li><br></ul><br>","remediationInstructions":"Remove the \"List\" permission from the AWS S3 configuration. Ensure that the \"Everyone\" grantee option is not enabled on the bucket unless the bucket content is intended to be publicly accessible<br><br>Browse to [subdomain].s3.amazonaws.com and see if data is returned.","name":"Excessive Privileges - AWS S3 Bucket - List Permission","sourceId":"M:eeea4113-6c6d-4461-a664-5b5e455ca671","severity":"Medium","businessImpact":"Risks associated with an attacker discovering a directory listing on the S3 bucket depend upon what type of what types of files are in the S3 bucket and what kind of permissions unauthorized users have to those files.","description":"AWS' Simple Storage Service (S3) allows users to register custom domains for their S3 buckets.  Misconfigured buckets will sometimes allow directory browsing, revealing the full file structure when the subdomain is directly browsed to."}}}