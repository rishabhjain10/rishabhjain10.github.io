{"default_url":null,"content":{"state":0,"taskInstructions":null,"references":[],"isTaskInstructionsVisible":true,"isCommentMandatory":false,"fields":[],"masterFinding":{"id":509,"exploitInstructions":"Burp will automatically parse the robots.txt file when it finds it, filling out the sitemap for the application. Additionally, it'll include an informational finding with the response from the server. You can review the response here, or browse to the file directly.\n\nAssemble a list of urls from the entries of the file and pass this file to Dirbuster or Burp Suite Intruder; review the responses for anything interesting.","verificationInstructions":"Review the contents of robots.txt. While robots.txt by itself is not a finding, robots.txt contents can sometimes be used to identify \"hidden\" application functionality such as administrative pages. Manually review all interesting content, then test and report the identified functionality using normal findings.\n\nReport this finding if there are any sensitive or vulnerable pages exposed by the robots.txt page.\n","remediationInstructions":"Review the contents of the site's robots.txt file, use Robots META tags instead of entries in the robots.txt file, and adjust the web server's access controls to limit access to sensitive material.\n","name":"Information Disclosure - Robots.txt","sourceId":"GID:2012","severity":"Informational","businessImpact":"An attacker can learn of directories containing potentially sensitive information by investigating the contents of the robots.txt file.","description":"Robots.txt files are intended to prevent automated web spiders from accessing certain directories on a web site for maintenance or indexing purposes. A malicious user may be able to use the contents of this file to learn the location of sensitive documents or directories on the affected site and either retrieve them directly or target them for other attacks.\n"}}}